[
  {
    "filename": "CV (Muhammad Ali Khokhar).pdf",
    "call_status": "done",
    "transcript": "interview:\nquestion 1: Can you share an experience where you had to integrate a machine learning model with the backend API, and how did you handle the deployment and serving of the model?\ncandidate: In my last project, I developed a recommendation engine using TensorFlow and integrated it with the backend API. Based on CPU utilization, I cached frequently requested predictions with Redis, which reduced API latency by almost 40%.\n\nquestion 2: How do you approach optimizing database queries for performance in a Python backend application, and can you give an example from your previous experience?\ncandidate: I usually start by analyzing slow queries using EXPLAIN in PostgreSQL. In one project, we optimized the user analytics API by adding proper indexing, which reduced query response time from 2.5 seconds to under 300 milliseconds. I also ensure connection pooling and use lazy loading in ORMs when appropriate.\n\nquestion 3: Have you worked with containerization using Docker, and can you walk me through your experience with Docker and how you used it in a project?\ncandidate: Yes, I've used Docker in several projects. In one, we built microservices for an e-commerce platform and used Docker Compose for local development. For production, we set up CI/CD pipelines with GitHub Actions and deployed images to AWS ECR. This setup ensured consistency between local and production environments.\n\nquestion 4: How do you ensure data integrity and consistency in a backend application, especially when working with large datasets, and can you share an example from your experience?\ncandidate: I focus on using database transactions and appropriate locking mechanisms. In a fintech project, we handled large transaction data by using ACID-compliant operations and row-level locking in PostgreSQL. We also implemented scheduled data consistency checks and automated alerts for anomalies, which helped us maintain accuracy even during heavy traffic periods.\n\nquestion 5: Can you describe a project where you had to collaborate with a frontend team to implement a new feature, and how did you handle communication and testing?\ncandidate: I worked on a social networking project with real-time notifications. I coordinated with the frontend team using Slack and Jira for task management. We held API contract meetings before starting development and set up a staging environment where both frontend and backend teams could test integrations together. This approach streamlined QA and reduced integration bugs by almost 70%."
  },
  {
    "filename": "Ahsan_Resume.pdf",
    "call_status": "done",
    "transcript": "interview:\\nquestion 1: Can you walk me through your experience with integrating trained ML models in scalable web applications using Fast API?\\ncandidate: Absolutely, in my last role, I integrated a sentiment analysis model into a Fast API backend. The model was served using it, and we still use it.\\n\\nquestion 2: How do you approach optimizing database queries and ensuring data integrity?\\ncandidate: I profile queries regularly, use indexing, and avoid N+1 query problems by optimizing our queries. For data integrity, I rely on transactions and constraints at the database level, and I also implement input validation at the API layer.\\n\\nquestion 3: Can you give an example of a time when you had to collaborate with the front-end to play a seamless API?\\ncandidate: Sure, for a dashboard, I worked closely with front-end developers. We documented the API endpoints using Swagger, held weekly syncs, and had a staging environment for early integration testing. This minimized bugs during production deployment.\\n\\nquestion 4: How do you stay up to date with the latest developments in AI or ML and back-end development?\\ncandidate: Proof of concept charts and part of customer support, which helps me understand prompt engineering better.\\n\\nquestion 5: Can you describe a project where you implemented background jobs in asynchronous processing?\\ncandidate: Yes, in an email automation platform, I used Celery for background tasks like sending bulk emails and processing analytics reports. AC processing allowed us to handle thousands of jobs concurrently without blocking API performance.\\n\\nquestion 6: How do you ensure your code is clean and well-documented?\\ncandidate: I follow PEP standards, write modular code, and document everything using docstrings in README files. For testing, I rely on Pytest and write unit and integration tests to cover key functionalities."
  },
  {
    "filename": "MilaAllenResume.pdf",
    "call_status": "done",
    "transcript": "interview:\\nquestion 1: Can you share an experience where you had to optimize database queries for better performance? What steps did you take?\\ncandidate: In one of my recent projects, we had performance issues with PostgreSQL queries fetching analytics data. I used EXPLAIN analysis and restructured a few joins to make them more efficient, which brought the query time down from almost 5 seconds to under 400 milliseconds.\\n\\nquestion 2: How do you approach designing and implementing scalable RESTful APIs using Python?\\ncandidate: I focus on clean, modular architecture, for example, with Fast API. I separate writers, services, and models for maintainability. I also add caching layers with Redis for high-traffic endpoints.\\n\\nquestion 3: What's your experience with Agile methodologies, and how did you apply them in your previous roles?\\ncandidate: I worked in an Agile environment for over 3 years at my last company. We used two-week sprints with daily stand-up meetings, which really helped improve delivery productivity and team collaboration.\\n\\nquestion 4: Can you tell me about a project where you had to integrate services or internal microservices in a fintech project, such as integration with a third-party payment gateway?\\ncandidate: The main challenge was handling timeouts and inconsistent responses. I implemented retry logic with exponential backoff and detailed logging, which improved reliability and reduced failed transactions by around 15%.\\n\\nquestion 5: How do you ensure data integrity and quality in your data workflows?\\ncandidate: I use validation checkpoint processes, for example, in one piece of Spark-based pipeline, we validated schema compliance, checked for duplicates, and ran automated tests before data was moved into production tables. This significantly reduced bad data making its way into analytics dashboards."
  },
  {
    "filename": "TimothyDuncanResume.pdf",
    "call_status": "done",
    "transcript": "interview:\\nquestion 1: Can you share an experience where you had to optimize database queries for better performance?\\ncandidate: In an E-Commerce platform, our checked API was taking too long. I optimized the PostgreSQL queries by creating composite indexes and caching frequently used results. This reduced average response times by over 50%.\\n\\nquestion 2: How do you insert data in your application?\\ncandidate: I use transactions and validate input on the server side. In one analytics project, we added checks and verification during data ingestion to detect corruption early.\\n\\nquestion 3: Have you worked with container deployment using Docker?\\ncandidate: Yes, extensively. I have used Docker for Python applications and Docker Compose for local development, and in production, I push images to Kubernetes clusters, ensuring smooth rollout and easy rollback strategies.\\n\\nquestion 4: How do you collaborate with front-end and DevOps teams for seamless deployments?\\ncandidate: Communication is key. I maintain detailed API documentation and schedule integration tests in staging before production.\\n\\nquestion 5: Can you tell me about a time you implemented synchronous message processing for real-time notifications without overloading the main API servers?\\ncandidate: In short, real-time notifications were implemented to prevent overloading the main API servers.\\n\\nquestion 6: How do you stay up to date with AI, machine learning, and deep learning?\\ncandidate: I follow online courses and attend webinars. Recently, I completed a course on Hugging Face Transformers and applied it in a project that automated an application for a legal tech company."
  },
  {
    "filename": "VioletRodriguezResume.pdf",
    "call_status": "done",
    "transcript": "interview:\\nquestion 1: Can you share an experience where you improved efficiency in a project?\\ncandidate: Yes, in a logistics platform, I redesigned the database schema and implemented indexing, which reduced query response times by about 30% and improved the overall performance of the system.\\n\\nquestion 2: How do you approach problem-solving in a team environment?\\ncandidate: Communication and brainstorming. Once, when our current server crashed, I facilitated a collaborative debugging session. We identified a memory leak in a third-party library and fixed it, reducing downtime by 30%.\\n\\nquestion 3: What do you think are the key benefits of using an API for building REST APIs?\\ncandidate: First API is fast, type-safe, and has an integrated programming language.\\n\\nquestion 4: How do you ensure data integrity when working with databases?\\ncandidate: I use proper foreign key constraints, unique indexes, and input validation. In one project, we used SQL Alchemy with PostgreSQL, which made transaction management seamless and reliable.\\n\\nquestion 5: Can you walk me through your experience with Docker, and how you maintain the environment 99% of the time during deployments?\\ncandidate: (No explicit response to Docker experience, but the candidate discusses their approach to learning and staying up-to-date) \\n\\nquestion 6: How do you stay up-to-date with industry developments, and can you share an example?\\ncandidate: I keep learning through online courses and community meetups. Recently, I applied predictive modeling techniques to build a tool that forecasted customer churn, reducing it by 15%."
  }
]